import os
#from mo_vector.client import MoVectorClient
from langchain.vectorstores.matrixone import Matrixone
from sentence_transformers import SentenceTransformer
import fitz
#from dotenv import load_dotenv
from transformers import pipeline

def prepare_model():
    global embed_model,embed_model_dims
    print("Downloading and loading the embedding model...")
    os.environ['HTTP_PROXY'] = 'http://127.0.0.1:1089'
    os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:1089'
    embed_model = SentenceTransformer("sentence-transformers/msmarco-MiniLM-L12-cos-v5", trust_remote_code=True)
    embed_model_dims = embed_model.get_sentence_embedding_dimension()

def text_to_embedding(text):
    """Generates vector embeddings for the given text."""
    embedding = embed_model.encode(text)
    return embedding.tolist()

# 生成 - 使用GPT-2模型生成回答
def generate_response(query,context):
    generator = pipeline("text-generation", model="gpt2", tokenizer="gpt2")
    response = generator(context, max_length=300, num_return_sequences=1, do_sample=True)[0]['generated_text']
    return response.strip()

#加载本地文档
def load_file():
    tmp_path = "/Users/heni/Downloads"
    pdf_file_name = "database_report.pdf"
    pdf_file_path = os.path.join(tmp_path,pdf_file_name)
    #将pdf转换为txt
    extracted_text = ""
    with fitz.open(pdf_file_path) as pdf_doc:  # 使用 pdf_data 创建 PDF 文档对象
        for page_num in range(pdf_doc.page_count):
            page = pdf_doc[page_num]
            extracted_text += page.get_text()

    # 将提取的文本保存为 .txt 文件
    local_text_file_name = "database_report.txt"
    output_text_file = os.path.join(tmp_path,local_text_file_name)
    with open(output_text_file, "w", encoding="utf-8") as text_file:
        text_file.write(extracted_text)
    return output_text_file

def read_in_chunks(file_object, chunk_size=64):
    """逐块读取文件内容"""
    while True:
        data = file_object.read(chunk_size)
        if not data:
            break
        yield data

#将文档分割成指定大小
def text_split():
    # 按块读取文件内容
    output_text_file = load_file()
    with open(output_text_file, 'r', encoding='utf-8') as file:
        i=0
        documents_list = []
        for chunk in read_in_chunks(file):
            documents = {
                    "id": i,
                    "text": chunk,
                    "embedding": text_to_embedding(chunk),
                    "metadata": {"category": "doc"},
                }
            i = i +1
            #print(documents)
            documents_list.append(documents)
    return documents_list

def mo_sdk_insert(MO,documents_list):
    MO.insert(
    ids=[doc["id"] for doc in documents_list],
    texts=[doc["text"] for doc in documents_list],
    embeddings=[doc["embedding"] for doc in documents_list],
    metadatas=[doc["metadata"] for doc in documents_list],
    )

def print_result(query, result):
    print(f"Search result (\"{query}\"):")
    for r in result:
        print(f"- text: \"{r.document}\", distance: {r.distance}")

def print_mix_result(query, keywords, result):
    print(f"Search result (\"{query}, {keywords}\"):")
    for r in result:
        print(f"- text: \"{r[1]}\", score: {r[0]}")

def print_full_text_result(keywords, result):
    print(f"Search result (keywords: {keywords}\"):")
    for r in result:
        print(f"- text: \"{r.document}\", score: {r.distance}")
    print("-----------------------------")

#insert检索数据
def retrieve_information():
    global vector_store
    #step 0: prepare embedding model env
    prepare_model()
    #step 1: connect mo
    #connection_string = f"mysql+pymysql://dump:111@127.0.0.1:6001/vector_db"
    MO = Matrixone(host="127.0.0.1", port=6001, user="dump", passwd="111", db="langchain_db", table_name="embedded_documents",
                   embedding=embedding)

#    vector_store = matrixone.MoVectorClient(
#        # The table which will store the vector data.
#        table_name='embedded_documents',
#        connection_string=connection_string,
#        # The dimension of the vector generated by the embedding model.
#        vector_dimension=embed_model_dims,
#        # Determine whether to recreate the table if it already exists.
#        drop_existing_table=True,
#    )
    #step 2: load pdf file and split small data
    documents_list=text_split()
    #step 3: embedding data and insert into mo table
    mo_sdk_insert(MO,documents_list)
    #step 4: create full text and embedding index
    MO.create_full_text_index()
    #step 5: 增强检索
    #search_result=mo_sdk_mix_query(query,keywords,rerank_option)
    #step 6: 使用gpt2生成回答
    #generate_response(query, search_result)

def test_rag_with_mix_query(MO):
    query = "什么是数据库"
    keywords = ["维护","存储"]
    rerank_option_rrf = {"rerank_type": "RRF", "rank_value": 60}
    rerank_option_weighted = {"rerank_type": "WeightedRank", "weighted_score": [0.8, 0.2], "rerank_score_threshold": 1}

    query_embedding = text_to_embedding(query)
    #混合查询rerank_option_rrf检索
    print(f"混合查询rerank_option_rrf检索结果:\n")
    search_result = MO.mix_query(query_embedding, keywords, rerank_option_rrf, k=3)
    print_mix_result(query, keywords, search_result)
    #混合查询rerank_option_weighted检索
    print(f"混合查询rerank_option_weighted检索结果:\n")
    search_result = MO.mix_query(query_embedding, keywords, rerank_option_weighted, k=3)
    print_mix_result(query, keywords, search_result)

def test_rag_with_embedding_query():
    query = "数据库的挑战是什么"
    query_embedding = text_to_embedding(query)
    print(f"Embedding l2检索结果:\n")
    search_result = vector_store.query(query_embedding, k=3)
    print_result(query, search_result)

def test_rag_with_embedding_distance_filter_query():
    query = "关系型数据库是什么"
    query_embedding = text_to_embedding(query)
    print(f"Embedding distance filter in range [1.0, 1.2]检索结果：\n")
    search_result = vector_store.query(query_embedding, k=3, dis_lower_bound=1.0, dis_upper_bound=1.2)
    print_result(query, search_result)

def test_rag_with_embedding_meta_filter_query():
    query = "非关系数据库采用什么架构"
    query_embedding = text_to_embedding(query)
    print(f"Vector query with meta filter检索结果: \n")
    search_result = vector_store.query(query_embedding, k=3, filter={"category": "\"report\""})
    print_result(query, search_result)

def test_rag_with_fulltext_query():
    keywords = ['传统关系型数据库中国产数据库市占率']
    print(f"Fulltext query检测结果: \n")
    search_result = vector_store.full_text_query(keywords, k=3)
    print_full_text_result(keywords, search_result)

def test_rag_with_fulltext_meta_filter_query():
    keywords = ['国产数据库情况']
    print(f"Fulltext meta过滤query检测结果: \n")
    search_result = vector_store.full_text_query(keywords, k=3, filter={"category": "\"data\""})
    print_full_text_result(keywords, search_result)

def test_rag_with_batch_query():
    #多个query
    query1 = "数据库可以分为什么"
    query_embedding1 = text_to_embedding(query1)

    query2 = "数据库性能指标"
    query_embedding2 = text_to_embedding(query2)

    print(f"Batch vector query检索结果：\n")
    search_results = vector_store.batch_query([query_embedding1, query_embedding2], k=3)
    for q, search_result in zip([query1, query2], search_results):
        print_result(q, search_result)

def test_rag():
    # 测试RAG多个查询场景
    retrieve_information()

    #混合查询rerank_option_weighted检索
    test_rag_with_mix_query()

if __name__ == "__main__":
    test_rag()
